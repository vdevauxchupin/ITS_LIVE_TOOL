{
 "cells": [
  {
   "cell_type": "raw",
   "id": "340bb56a",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: setup.html\n",
    "title: '04: access_setup -- Data setup + prep'\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8fb8bc66-030c-4580-95bb-e8ec541d0848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp obj_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f39676ad-3c6d-4aa3-b710-fc019f6cd69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev \n",
    "from nbdev import nbdev_export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef6d3d-7aa5-428c-99df-e9251ca3756e",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "935cd9af-35cf-4e8e-8c9f-87c1a5c50569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import tarfile\n",
    "from owslib.wfs import WebFeatureService\n",
    "from requests import Request\n",
    "\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely import geometry\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import xarray as xr\n",
    "import rioxarray as rio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "import s3fs\n",
    "# to get and use geojson datacube catalog\n",
    "import logging\n",
    "\n",
    "# for timing data access\n",
    "import time\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import s3fs as s3\n",
    "# for datacube xarra\n",
    "from pyproj import Transformer\n",
    "\n",
    "# for plotting time series\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import ipyleaflet as ipyl\n",
    "import ipywidgets as ipyw\n",
    "from ipywidgets import HTML\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d483c241-c631-4c02-8598-819b400d7be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from ITS_LIVE_TOOL import datacube_tools, interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19783ed-6583-421e-b74a-07176ec3227e",
   "metadata": {},
   "source": [
    "## Defining classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bcd3e2e-b6af-493b-bbd8-4c3d07ac8cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def point_to_gdf(point_ls):\n",
    "    '''\n",
    "    creates a geodataframe from a given point\n",
    "\n",
    "    input: list of [x,y] coords\n",
    "    output: geopandas gdf of point, in epsg:4326\n",
    "    '''\n",
    "        \n",
    "    d = {'x': point_ls[0],\n",
    "         'y': point_ls[1]}\n",
    "    df = pd.DataFrame(d, index=[0])\n",
    "    gdf = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df.x, df.y, crs='EPSG:4326'))\n",
    "    return gdf\n",
    "\n",
    "def remove_empty_timesteps(ds):\n",
    "    ds['step_count'] = (('mid_date'), range(len(ds['mid_date'])))\n",
    "    ds = ds.swap_dims({'mid_date':'step_count'})\n",
    "    time_step_keep = list(ds.v.dropna(how='all',dim='step_count').step_count.data)\n",
    "    ds_subset = ds.where(ds.step_count.isin(time_step_keep), drop=True)\n",
    "    ds_subset = ds_subset.swap_dims({'step_count':'mid_date'})\n",
    "    \n",
    "    return ds_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78cf69e0-68a1-4cb8-acc0-d779c26f0fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Glacier_Centerline():\n",
    "    '''class to hold all data associated with a centerline'''\n",
    "    def __init__(self, name, rgi_id):\n",
    "        self.name = name\n",
    "        self.rgi_id = rgi_id\n",
    "        self.rgi_region = rgi_id.split('-')[1].split('.')[0]\n",
    "        self._centerline_path = self._download_centerlines()\n",
    "        self.centerlines, self.main_centerline, self.utm_zone = self._add_centerlines()\n",
    "\n",
    "    def _download_centerlines(self, dest_folder = os.getcwd()):\n",
    "        dest_folder = dest_folder.split('nbs')[0]+'centerlines/'\n",
    "    \n",
    "        #this first part of htis function is scraping the urls for OGGM centerlines for each RGI region from the summary page\n",
    "        # and organizing them into a dict \n",
    "        # hardcoded -- this is the link to OGGM centerlines separated into rgi regions -- each is compressed as tar.gz\n",
    "        orig_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/centerlines/RGI62/b_010/L2/summary/'\n",
    "        response = requests.get(orig_url)\n",
    "        link_header = orig_url.split('~oggm')[0][:-1] #isolate just the beginning\n",
    "            \n",
    "        #print('link header :', link_header)\n",
    "        #link_header = 'https://cluster.klima.uni-bremen.de'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        header = soup.find('h1') #this points to the summary page containing links for all regions \n",
    "        gen_link = str(header).split(' ')[2].split('<')[0]\n",
    "        data_url_gen = link_header + gen_link + '/' #this is the full url to the summary paeg\n",
    "        links = soup.find_all('a') #find all the links contained in page\n",
    "        \n",
    "        smoothed_flag = 'smoothed' # want only 'centerlines', not smoothed centerlines -- can change this\n",
    "        region_ls, region_url_ls = [],[]\n",
    "        \n",
    "        for link in range(len(links)): #this loop creates a dict where each key is an rgi region and each value is the url to that regions oggm centerlines\n",
    "            if links[link].attrs['href'].startswith('centerlines'):\n",
    "        \n",
    "                if smoothed_flag not in links[link].attrs['href']:\n",
    "                    region = links[link]['href'].split('_')[1].split('.')[0]\n",
    "                    region_url = data_url_gen+ links[link]['href']\n",
    "                    region_ls.append(region)\n",
    "                    region_url_ls.append(region_url)\n",
    "                    \n",
    "                else:\n",
    "                    pass\n",
    "        \n",
    "        region_url_dict = dict(zip(region_ls, region_url_ls))\n",
    "        rgi_region_code = self.rgi_region\n",
    "    \n",
    "        region_centerline_url = region_url_dict[rgi_region_code]\n",
    "        #print('url for specified region : ', region_centerline_url)\n",
    "        \n",
    "        #download_extract(region_centerline_url, dest_folder = dest_folder)\n",
    "    \n",
    "        #the second part of this function is reading the specified url, and downloading + extracting the file to a specified location\n",
    "        \n",
    "        #help from https://stackoverflow.com/questions/56950987/download-file-from-url-and-save-it-in-a-folder-python\n",
    "        if not os.path.exists(dest_folder):\n",
    "            os.makedirs(dest_folder)\n",
    "    \n",
    "        filename = region_centerline_url.split('/')[-1].replace(\" \", \"_\")\n",
    "        file_path = os.path.join(dest_folder, filename)\n",
    "        #print(file_path)\n",
    "    \n",
    "        r = requests.get(region_centerline_url, stream=True)\n",
    "        if r.ok:\n",
    "            #print('saving to: ', os.path.abspath(file_path))\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=1024*8):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        f.flush()\n",
    "                        os.fsync(f.fileno())\n",
    "                    else:\n",
    "                        print('donwload failed')\n",
    "    \n",
    "        \n",
    "        file = tarfile.open(file_path)\n",
    "        file.extractall(dest_folder)\n",
    "    \n",
    "        #return the path to the shp file\n",
    "        a = file_path.split('tar.gz')[0] + 'shp'\n",
    "        #print(a)\n",
    "        return a\n",
    "\n",
    "    def _add_centerlines(self):\n",
    "        gpdf = gpd.read_file(self._centerline_path)\n",
    "\n",
    "        gpdf = gpdf.loc[gpdf['RGIID'] == self.rgi_id]\n",
    "        utm = str(gpdf.estimate_utm_crs())\n",
    "        \n",
    "        gpdf_main = gpdf.loc[gpdf['MAIN'] == 1].to_crs(utm)\n",
    "        gpdf_all = gpdf.to_crs(utm)\n",
    "        return gpdf_all, gpdf_main, utm\n",
    "\n",
    "    def sample_n_points(self, n ):\n",
    "    #help from https://stackoverflow.com/questions/62990029/how-to-get-equally-spaced-points-on-a-line-in-shapely\n",
    "\n",
    "        distances = np.linspace(0, self.main_centerline.length*0.90, n)\n",
    "        points = [self.main_centerline.interpolate(distance) for distance in distances]\n",
    "        multipoint = unary_union(points)\n",
    "        labels = [f'point {i}' for i in range(n)]\n",
    "        coords = [(p.x, p.y) for p in multipoint.geoms]\n",
    "        xs = [coords[i][0] for i in range(len(coords))]\n",
    "        ys = [coords[i][1] for i in range(len(coords))]\n",
    "        df = pd.DataFrame({'label':labels,\n",
    "                       'x':xs,\n",
    "                       'y':ys})\n",
    "        gdf = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df['x'], df['y'])).set_crs(self.utm_zone)\n",
    "        return gdf\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb6c43d0-8dbd-4e30-87ff-a04acd63f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Glacier():\n",
    "    '''class to hold all data associated with individual glacier\n",
    "    inputs: name (str), rgi_id (str), working_dir_path (str, where oggm data should be written)\n",
    "    url (str, url to oggm prepro data), centerline path (str, path to locally stored \n",
    "    centerline data (**wanted to have this not rely on local data but haven't gotten that working yet**)\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, name, rgi_id, utm_crs):\n",
    "\n",
    "        self.name = name\n",
    "        self.rgi_id = rgi_id\n",
    "        self._rgi_region = rgi_id.split('-')[1].split('.')[0]\n",
    "        self._outline = self._download_rgi()\n",
    "        self.utm_zone = utm_crs\n",
    "        self.outline_prj = self._outline.to_crs(self.utm_zone)\n",
    "        #self.utm_zone = str(self.outline.estimate_utm_crs())\n",
    "        \n",
    "    def _download_rgi(self):\n",
    "\n",
    "        region = self._rgi_region\n",
    "    \n",
    "        rgi_region_dict = {'01': 'GLIMS:RGI_Alaska', '02':  'GLIMS:RGI_WesternCanadaUS', '03':  'GLIMS:RGI_ArcticCanadaNorth',\n",
    "                       '04': 'GLIMS:RGI_ArcticCanadaSouth', '05':  'GLIMS:RGI_GreenlandPeriphery', '06': 'GLIMS:RGI_Iceland',\n",
    "                       '07':  'GLIMS:RGI_Svalbard', '08':  'GLIMS:RGI_Scandinavia', '09': 'GLIMS:RGI_RussianArctic', \n",
    "                       '10': 'GLIMS:RGI_NorthAsia', '11':  'GLIMS:RGI_CentralEurope', '12':  'GLIMS:RGI_CaucasusMiddleEast',\n",
    "                       '13':   'GLIMS:RGI_CentralAsia', '14': 'GLIMS:RGI_SouthAsiaWest', '15': 'GLIMS:RGI_SouthAsiaEast',\n",
    "                       '16':  'GLIMS:RGI_LowLatitudes', '17': 'GLIMS:RGI_SouthernAndes', '18': 'GLIMS:RGI_NewZealand',\n",
    "                       '19':  'GLIMS:RGI_AntarcticSubantarctic'}\n",
    "    \n",
    "        rgi_region_name = rgi_region_dict[region]\n",
    "    \n",
    "        rgi_url = \"https://www.glims.org/geoserver/ows?service=wms&version=1.3.0&request=GetCapabilities\"\n",
    "       \n",
    "        wfs = WebFeatureService(url=rgi_url,  version = \"2.0.0\")\n",
    "        \n",
    "        layers = list(wfs.contents)\n",
    "    \n",
    "        layer = [layers[i] for i in range(len(layers)) if layers[i] == rgi_region_name][0]\n",
    "        response = wfs.getfeature(typename = layer, outputFormat='SHAPE-ZIP')\n",
    "        data = gpd.read_file(response)\n",
    "    \n",
    "        data_glacier = data.loc[data['RGIID'] == self.rgi_id]\n",
    "        \n",
    "        return data_glacier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25c3112b-fc86-4321-9198-f4efec68e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class Glacier_Point():\n",
    "\n",
    "    def __init__(self, name, label, rgi_id, point_coords_latlon, var_ls):\n",
    "\n",
    "        self.name = name\n",
    "        self.label = label\n",
    "        self.rgi_id = rgi_id\n",
    "        #self.utm_crs = utm_crs\n",
    "        #self.glacier_gridded_data = glacier_obj.utm_gridded_data\n",
    "        #self.glacier_centerline = glacier_obj.centerline_main\n",
    "        self.point_latlon = point_coords_latlon\n",
    "        self.point_gdf = self.point_to_gdf()\n",
    "        #self.utm_crs = str(self.point_gdf.estimate_utm_crs())\n",
    "        self.datacube_point = self._add_image_pair_point(var_ls)\n",
    "        self.datacube_sub = self._add_image_pair_subcube(var_ls)\n",
    "        self.utm_crs = str(self.datacube_point.rio.crs)\n",
    "        #self.padded_centerline_subcube = self._extract_subcube_along_padded_centerline()\n",
    "        #self.TRIM_padded_centerline_subcube = self._subset_ds_by_sensor_baseline('cl')\n",
    "        self.cube_around_point = self._extract_3x3_cube_around_point()\n",
    "        self.point_v = self._calc_point_v()\n",
    "        #self.TRIM_subcube = self._subset_ds_by_sensor_baseline('dc_full')\n",
    "\n",
    "        #self.TRIM_cube_around_point = self._subset_ds_by_sensor_baseline('dc_cube')\n",
    "\n",
    "    def point_to_gdf(self):\n",
    "        \n",
    "        d = {'x': self.point_latlon[0],\n",
    "             'y':self.point_latlon[1]}\n",
    "        df = pd.DataFrame(d, index=[0])\n",
    "        gdf = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df.x, df.y, crs='EPSG:4326'))\n",
    "        return gdf\n",
    "\n",
    "    def _point_v_mosaic(self):\n",
    "\n",
    "        point_utm = self.point_gdf.to_crs(self.utm_crs)\n",
    "        \n",
    "        ds_clip = self.glacier_gridded_data.rio.clip(point_utm.geometry, point_utm.crs)\n",
    "        return ds_clip\n",
    "            \n",
    "    def _add_image_pair_point(self, var_ls):\n",
    "\n",
    "        dc = datacube_tools.DATACUBETOOLS()\n",
    "        var_ls = var_ls \n",
    "        dc_point_full = dc.get_timeseries_at_point(self.point_latlon, point_epsg_str = '4326', variables = var_ls)\n",
    "        dc_point = dc_point_full[1]\n",
    "        crs = f\"EPSG:{dc_point.mapping.attrs['spatial_epsg']}\"\n",
    "        dc_point = dc_point.rio.write_crs(crs)\n",
    "        #dc_point = dc_point.rio.write_nodata(np.nan)\n",
    "        dc_point = dc_point.dropna(how='any', dim='mid_date')\n",
    "\n",
    "    \n",
    "        dc_point['acquisition_date_img1'] = (('mid_date'), pd.to_datetime(dc_point.acquisition_date_img1))\n",
    "        dc_point['acquisition_date_img2'] = (('mid_date'), pd.to_datetime(dc_point.acquisition_date_img2))\n",
    "    \n",
    "        dc_point['img_separation'] = -1*((dc_point.acquisition_date_img1 - dc_point.acquisition_date_img2).astype('timedelta64[D]') / np.timedelta64(1,'D'))\n",
    "\n",
    "        return dc_point\n",
    "\n",
    "    def remove_empty_timesteps(ds):\n",
    "        ds['step_count'] = (('mid_date'), range(len(ds['mid_date'])))\n",
    "        ds = ds.swap_dims({'mid_date':'step_count'})\n",
    "        time_step_keep = list(ds.v.dropna(how='all',dim='step_count').step_count.data)\n",
    "        ds_subset = ds.where(ds.step_count.isin(time_step_keep), drop=True)\n",
    "        ds_subset = ds_subset.swap_dims({'step_count':'mid_date'})\n",
    "\n",
    "        return ds_subset\n",
    "\n",
    "    def _add_image_pair_subcube(self, var_ls):\n",
    "\n",
    "        dc = datacube_tools.DATACUBETOOLS()\n",
    "        var_ls = var_ls\n",
    "        dc_full_sub = dc.get_subcube_around_point(self.point_latlon, point_epsg_str = '4326', variables=var_ls)\n",
    "        crs = f\"EPSG:{dc_full_sub[0].mapping.attrs['spatial_epsg']}\"\n",
    "        dc_sub = dc_full_sub[1]\n",
    "        dc_sub = dc_sub.rio.write_crs(crs)\n",
    "        #dc_sub = dc_sub.rio.write_nodata(np.nan)\n",
    "        dc_sub = dc_sub.dropna(how='all', dim='mid_date')\n",
    "        dc_sub['acquisition_date_img1'] = (('mid_date'), pd.to_datetime(dc_sub.acquisition_date_img1))\n",
    "        dc_sub['acquisition_date_img2'] = (('mid_date'), pd.to_datetime(dc_sub.acquisition_date_img2))\n",
    "    \n",
    "        dc_sub['img_separation'] = -1*((dc_sub.acquisition_date_img1 - dc_sub.acquisition_date_img2).astype('timedelta64[D]') / np.timedelta64(1,'D'))\n",
    "\n",
    "        dc_sub = remove_empty_timesteps(dc_sub)\n",
    "        \n",
    "        return dc_sub\n",
    "\n",
    "    def _extract_subcube_along_padded_centerline(self, pad=200):\n",
    "        \n",
    "        cl = self.glacier_centerline.to_crs(self.utm_crs)\n",
    "        line = shapely.geometry.LineString(cl.get_coordinates().loc[:,['x','y']].values)\n",
    "        PAD = pad #meters\n",
    "        line_buf = gpd.GeoSeries([line], crs=self.utm_crs).buffer(PAD, cap_style=2)\n",
    "        padded_cl_gdf = gpd.GeoDataFrame({'id':self.label,\n",
    "                                  'padding':120}, index=[0], geometry=line_buf)\n",
    "        glacier_subcube_cl = self.datacube_sub.rio.clip(padded_cl_gdf.geometry, padded_cl_gdf.crs)\n",
    "        return glacier_subcube_cl\n",
    "        \n",
    "    def _extract_3x3_cube_around_point(self):\n",
    "    \n",
    "        padded_point = gpd.GeoDataFrame({'id':self.label}, \n",
    "                                index=[0],\n",
    "                                geometry = self.point_gdf.to_crs(self.utm_crs).buffer(distance=200))\n",
    "        dc = self.datacube_sub.rio.clip(padded_point.geometry, padded_point.crs)\n",
    "\n",
    "        return dc\n",
    "\n",
    "    def _calc_point_v(self):\n",
    "        med_v = self.cube_around_point.where(self.cube_around_point.img_separation >= 365, drop=True).v.median(dim=['x','y','mid_date'])\n",
    "        return med_v\n",
    "\n",
    "    def _subset_ds_by_sensor_baseline(self, format):\n",
    "        \n",
    "        min_tb_df = calc_min_tbaseline(self)\n",
    "        \n",
    "        #split ds by sensor (sensor options are hardcoded, will need to update when rest of landsat added \n",
    "        if format == 'cl': \n",
    "            l8 = self.padded_centerline_subcube.where(self.padded_centerline_subcube.satellite_img1 == '8.0',drop=True)\n",
    "            l9 = self.padded_centerline_subcube.where(self.padded_centerline_subcube.satellite_img1 == '9.0',drop=True)\n",
    "            s1 = self.padded_centerline_subcube.where(self.padded_centerline_subcube.satellite_img1.isin(['1A','1B']),drop=True)\n",
    "            s2 = self.padded_centerline_subcube.where(self.padded_centerline_subcube.satellite_img1.isin(['2A','2B']),drop=True)\n",
    "        elif format == 'dc_full':\n",
    "            l8 = self.datacube_sub.where(self.datacube_sub.satellite_img1 == '8.0',drop=True)\n",
    "            l9 = self.datacube_sub.where(self.datacube_sub.satellite_img1 == '9.0',drop=True)\n",
    "            s1 = self.datacube_sub.where(self.datacube_sub.satellite_img1.isin(['1A','1B']),drop=True)\n",
    "            s2 = self.datacube_sub.where(self.datacube_sub.satellite_img1.isin(['2A','2B']),drop=True)\n",
    "            \n",
    "        elif format == 'dc_cube':\n",
    "            l8 = self.cube_around_point.where(self.datacube_sub.satellite_img1 == '8.0',drop=True)\n",
    "            l9 = self.cube_around_point.where(self.datacube_sub.satellite_img1 == '9.0',drop=True)\n",
    "            s1 = self.cube_around_point.where(self.datacube_sub.satellite_img1.isin(['1A','1B']),drop=True)\n",
    "            s2 = self.cube_around_point.where(self.datacube_sub.satellite_img1.isin(['2A','2B']),drop=True)\n",
    "    \n",
    "        l8_sub = l8.where(l8.img_separation >= int(min_tb_df.loc[min_tb_df['sensor'] == 'L8']['min_tb (days)']), drop=True)\n",
    "        l9_sub = l9.where(l9.img_separation >= int(min_tb_df.loc[min_tb_df['sensor'] == 'L9']['min_tb (days)']), drop=True)\n",
    "        s1_sub = s1.where(s1.img_separation >= int(min_tb_df.loc[min_tb_df['sensor'] == 'S1']['min_tb (days)']), drop=True)\n",
    "        s2_sub = s2.where(s2.img_separation >= int(min_tb_df.loc[min_tb_df['sensor'] == 'S2']['min_tb (days)']), drop=True)\n",
    "        ds_ls = [l8_sub, l9_sub, s1_sub, s2_sub]\n",
    "        concat_ls = []\n",
    "        for ds in range(len(ds_ls)):\n",
    "            if len(ds_ls[ds].mid_date) > 0:\n",
    "                concat_ls.append(ds_ls[ds])\n",
    "        print(len(concat_ls))\n",
    "        try:\n",
    "            combine = xr.concat(concat_ls, dim='mid_date')\n",
    "            combine = combine.sortby(combine.mid_date)\n",
    "            return combine\n",
    "        except:\n",
    "            print('something went wrong')\n",
    "                         \n",
    "def calc_min_tbaseline(Point):\n",
    "    med_v = Point.point_v.data\n",
    "    gsd_s2, gsd_l8, gsd_s1, gsd_l9 = 10,15, 10, 15\n",
    "    name_ls = ['S2','L8','S1', 'L9']\n",
    "    gsd_ls = [gsd_s2, gsd_l8, gsd_s1, gsd_l9]\n",
    "    sensor_str_ls = [['2A','2B'], '8.0',['1A','1B'],'9.0']\n",
    "    min_tb_ls = []\n",
    "    for element in range(len(gsd_ls)):\n",
    "        min_tb = ((gsd_ls[element]*2)/med_v)*365\n",
    "        min_tb_ls.append(min_tb)\n",
    "        #print(min_tb, ' days')\n",
    "\n",
    "    min_tb_dict= {'sensor':name_ls, \n",
    "                  'gsd': gsd_ls, \n",
    "                  'min_tb (days)': min_tb_ls,\n",
    "                 'sensor_str':sensor_str_ls}\n",
    "    df = pd.DataFrame(min_tb_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c82f6a-b5f2-4953-a566-6f7462dbb2a4",
   "metadata": {},
   "source": [
    "## Examples - baltoro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaaf769-fc3a-44ea-91ee-54fef537b81b",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2fb9d19-ee2c-4e22-a146-3e53721190b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#working dir path for oggm \n",
    "wd_path = '/uufs/chpc.utah.edu/common/home/u1269862/2023_fall/oggm_scratch'\n",
    "l12_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/centerlines/'\n",
    "#ind glacier inputs \n",
    "rgi_id = 'RGI60-14.06794'\n",
    "point = [76.3797, 35.7376]\n",
    "label = 'mid-glacier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77f63340-b363-4b06-90fe-9e77a730066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map = interactive.Widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6e21b5e-b947-445d-8480-bf761b32eee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5291621210fe4d14a4e1e0af3b64d112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Map(center=[0, 0], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', â€¦"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_map.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "293a6502-92d8-4d95-bd85-5180ebbf5baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 glaciers selected\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "try: \n",
    "    coords, gpdf, urls = interactive.return_clicked_info(data_map)\n",
    "    point = [coords[0][1], coords[0][0]]\n",
    "\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f98a5299-c83d-4c04-bb40-46b3708a9c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RGI60-02.02424'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "gpdf[0]['RGIID'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a20bf18-cb54-418b-a111-1a3b5cb99f33",
   "metadata": {},
   "source": [
    "### Glacier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2db921c3-9eca-44d0-9106-b96c32be634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "var_ls = ['v','vy','vx','v_error','mapping','satellite_img1','satellite_img2','acquisition_date_img1', 'acquisition_date_img2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7281e9a-362e-4cb4-a102-02fcec7e1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "glaicer_lw = Glacier('shimo', gpdf[0]['RGIID'].iloc[0], 'EPSG:32645')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1315b32b-1527-4bfe-8315-0e04e7d66a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original xy [-124.0492954983568, 50.79377601571208] 4326 maps to datacube (426046.2746193191, 5627417.167637969) EPSG:32610\n",
      "original xy [-124.0492954983568, 50.79377601571208] 4326 maps to datacube (426046.2746193191, 5627417.167637969) EPSG:32610\n",
      "subset and load at  71.63 seconds\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "glacier_pt = Glacier_Point(glaicer_lw.name, 'mid ish', glaicer_lw.rgi_id, [coords[0][1], coords[0][0]], var_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c920dc58-5088-4cd7-aac2-9677b3e0d83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1269862/2023_fall/ITS_LIVE_TOOL/centerlines/centerlines_02.shp\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "glacier_centerline = Glacier_Centerline(glaicer_lw.name, glaicer_lw.rgi_id)\n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bc762-a9b1-4f72-a3c9-eaafd695b173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
